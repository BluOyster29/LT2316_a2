{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(folder):\n",
    "    #needs args\n",
    "    train_df = pd.read_csv(folder + 'split/train.csv')\n",
    "    val_df = pd.read_csv(folder + 'split/validation.csv')\n",
    "    test_df = pd.read_csv(folder + 'split/test.csv')\n",
    "    return train_df, val_df, test_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data_frame):\n",
    "    counter = 0\n",
    "    sent1_tokenized = []\n",
    "    tenp = len(data_frame) / 5\n",
    "    ten = 0\n",
    "    labels = []\n",
    "    print(\"Fetching Labels\")\n",
    "    for i in data_frame['label']:\n",
    "        labels.append(i)\n",
    "    \n",
    "    print(\"Tokenizing sentence 1\")\n",
    "    for i in data_frame['sent1']:\n",
    "        if counter % tenp == 0:\n",
    "            print(str(ten) + '% tokenized')\n",
    "            ten += 10\n",
    "        sent1_tokenized.append([re.sub('-',' ',x.lower()) for x in word_tokenize(i)])\n",
    "        counter +=1 \n",
    "\n",
    "    sent2_tokenized = []\n",
    "    counter = 0\n",
    "    print(\"Tokenizing sentence 2\")\n",
    "    for i in data_frame['sent2']:\n",
    "        if counter % tenp == 0:\n",
    "            print(str(ten) + '% tokenized')\n",
    "            ten += 10\n",
    "        sent2_tokenized.append([re.sub('-',' ',x.lower()) for x in word_tokenize(i)])\n",
    "        counter +=1 \n",
    "\n",
    "    print(\"Tokens Generated\")\n",
    "    \n",
    "    \n",
    "    return sent1_tokenized, sent2_tokenized, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_sent):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words += string.punctuation\n",
    "    for i in tokenized_sent:\n",
    "        if i in stop_words or type(i) == int:\n",
    "            tokenized_sent.remove(i)\n",
    "\n",
    "    return tokenized_sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(sent1_tokenized, sent2_tokenized):\n",
    "    counter = 0\n",
    "    ten = 0\n",
    "    tenper = len(sent1_tokenized) / 5\n",
    "\n",
    "    print('Post Processing sentence 1')\n",
    "    for i in sent1_tokenized:\n",
    "        counter += 1\n",
    "        if counter % tenper == 0:\n",
    "            print(str(ten) + '% processed')\n",
    "            ten += 10\n",
    "        i = remove_stopwords(i)\n",
    "\n",
    "    print('Post Processing sentence 2')\n",
    "    counter = 0\n",
    "    for i in sent2_tokenized:\n",
    "        counter += 1\n",
    "        if counter % tenper == 0:\n",
    "            print(str(ten) + '% processed')\n",
    "            ten += 10\n",
    "        i = remove_stopwords(i)\n",
    "        \n",
    "    print('Finished')\n",
    "    \n",
    "    return sent1_tokenized, sent2_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocab(sent1_tokenz, sent2_tokenz):\n",
    "    raw_text = []\n",
    "    for i in sent1_tokenz:\n",
    "        raw_text += i\n",
    "    for i in sent2_tokenz:\n",
    "        raw_text += i\n",
    "    \n",
    "    int2char = dict(enumerate(set(raw_text)))\n",
    "    vocab = {char : num for num, char in int2char.items()}\n",
    "    return vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = read_csv('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Labels\n",
      "Tokenizing sentence 1\n",
      "0% tokenized\n",
      "10% tokenized\n",
      "20% tokenized\n",
      "30% tokenized\n",
      "40% tokenized\n",
      "Tokenizing sentence 2\n",
      "50% tokenized\n",
      "60% tokenized\n",
      "70% tokenized\n",
      "80% tokenized\n",
      "90% tokenized\n",
      "Tokens Generated\n"
     ]
    }
   ],
   "source": [
    "train_sent1_tokenized, train_sent2_tokenized, labels = tokenize(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'punctuation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-c93acdd50369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'punctuation'"
     ]
    }
   ],
   "source": [
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Processing sentence 1\n",
      "0% processed\n",
      "10% processed\n",
      "20% processed\n",
      "30% processed\n",
      "40% processed\n",
      "Post Processing sentence 2\n",
      "50% processed\n",
      "60% processed\n",
      "70% processed\n",
      "80% processed\n",
      "90% processed\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "proc_sent1, proc_sent2 = post_processing(train_sent1_tokenized, train_sent2_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = gen_vocab(train_sent1_tokenized, train_sent2_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1],[2,3],[3,2,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [[3,4],[3,4,5],[6,6,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [[0],[0],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(a,b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(sent1,sent2,labels):\n",
    "    df = pd.DataFrame(data=list(zip(sent1,sent2,labels)), columns=['sent1','sent2','labels'])\n",
    "    df.to_csv('data/processed_training.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df = create_df(proc_sent1, proc_sent2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipping_data(df):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for idx, row in df.iterrows():\n",
    "        sent_1 = row['sent1']\n",
    "        sent_2 = row['sent2']\n",
    "        X_train.append((sent_1,sent_2))\n",
    "        y_train.append(row['labels'])\n",
    "        \n",
    "    return X_train, y_train\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = zipping_data(processed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebatesSet(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DebatesSet(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'vectors/gLove/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {w: vectors[word2idx[w]] for w in words}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.zeros((matrix_len, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_found = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = np.array(torch.Tensor(weights_matrix).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63819,    50])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(target_vocab)\n",
    "weights_matrix = np.zeros((matrix_len, 50))\n",
    "words_found = 0\n",
    "\n",
    "glove_vocab = {}\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        glove_vocab[word] = glove[word]\n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(50,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings = len(weights_matrix)\n",
    "    embedding_dim = 50\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer, num_embeddings, embedding_dim = create_emb_layer(torch.Tensor(weights_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('pretrain_vectors/glove/glove_vocab.pkl','wb')\n",
    "pickle.dump(glove_vocab,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
